import os
import json
import asyncio
import logging
import numpy as np
import requests
import torch
import aiohttp
import ollama
from typing import Dict, List, Optional, Tuple
from pymongo import MongoClient
from dotenv import load_dotenv
from transformers import AutoModelForSequenceClassification, AutoTokenizer

# ä½¿ç”¨FastMCPæœåŠ¡å™¨
from mcp.server.fastmcp import FastMCP

# -------------------------- åŸºç¡€é…ç½® --------------------------
# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    encoding='utf-8'
)
logger = logging.getLogger("bio-invasion-mcp")

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()
ENV = os.getenv("MCP_ENV", "development")

# -------------------------- å…¨å±€èµ„æºåˆå§‹åŒ– --------------------------
# 1. å¼‚æ­¥HTTPä¼šè¯ï¼ˆæ‡’åŠ è½½ï¼‰
aiohttp_session = None

# 2. MongoDBè¿æ¥
def init_mongo():
    # ä½¿ç”¨æ–°çš„ç¯å¢ƒå˜é‡å‘½åçº¦å®š
    mongo_uri = os.getenv("MONGO_URI", "mongodb://localhost:27017/")
    db_name = os.getenv("MONGO_DB_NAME", "ä¸­å›½ç”Ÿç‰©å…¥ä¾µç ”ç©¶")
    col_name = os.getenv("MONGO_COLLECTION", "ç”Ÿç‰©å…¥ä¾µç ”ç©¶")
    try:
        client = MongoClient(mongo_uri, serverSelectionTimeoutMS=5000)
        client.admin.command("ping")  # éªŒè¯è¿æ¥
        db = client[db_name]
        col = db[col_name]
        logger.info(f"âœ… MongoDBè¿æ¥æˆåŠŸï¼š{db_name}.{col_name}")
        return client, col
    except Exception as e:
        logger.error(f"âŒ MongoDBåˆå§‹åŒ–å¤±è´¥ï¼š{str(e)}")
        # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå…è®¸æœåŠ¡å™¨åœ¨æ²¡æœ‰MongoDBçš„æƒ…å†µä¸‹å¯åŠ¨ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        return None, None

mongo_client, mongo_col = init_mongo()

# 3. é‡æ’åºæ¨¡å‹ï¼ˆæ‡’åŠ è½½ï¼‰
reranker_model = None
reranker_tokenizer = None  
def init_reranker():
    global reranker_model, reranker_tokenizer
    model_name = os.getenv("RERANKER_MODEL", "BAAI/bge-reranker-large")
    try:
        reranker_tokenizer = AutoTokenizer.from_pretrained(model_name) 
        reranker_model = AutoModelForSequenceClassification.from_pretrained(model_name)
        reranker_model.eval()
        logger.info(f"âœ… é‡æ’åºæ¨¡å‹åŠ è½½æˆåŠŸï¼š{model_name}")
    except Exception as e:
        logger.warning(f"âš ï¸  é‡æ’åºæ¨¡å‹åŠ è½½å¤±è´¥ï¼š{str(e)}ï¼ˆå°†ç”¨åŸç”Ÿå‘é‡æ’åºï¼‰")  

# åˆå§‹åŒ–é‡æ’åºæ¨¡å‹
init_reranker()

# -------------------------- æ ¸å¿ƒå·¥å…·å‡½æ•° --------------------------  
async def get_embedding(text: str) -> List[float]:
    """ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡ï¼ˆåŸºäºæœ¬åœ°Ollamaæ¨¡å‹ï¼‰"""
    embedding_model = os.getenv("EMBEDDING_MODEL", "qwen3-embedding:8b")
    
    try:
        # ç›´æ¥ä½¿ç”¨æœ¬åœ°Ollamaæ¨¡å‹
        response = ollama.embeddings(model=embedding_model, prompt=text)
        logger.info(f"âœ… ä½¿ç”¨æœ¬åœ°åµŒå…¥æ¨¡å‹æˆåŠŸ: {embedding_model}")
        return response["embedding"]
    except Exception as e:
        logger.warning(f"âš ï¸  ä¸»åµŒå…¥æ¨¡å‹ {embedding_model} å¤±è´¥: {str(e)}ï¼Œå°è¯•å¤‡ç”¨æ¨¡å‹")
        
        # å›é€€åˆ°nomic-embed-textæ¨¡å‹
        try:
            response = ollama.embeddings(model="nomic-embed-text", prompt=text)
            logger.info(f"âœ… ä½¿ç”¨å¤‡ç”¨åµŒå…¥æ¨¡å‹æˆåŠŸ")
            return response["embedding"]
        except Exception as fallback_error:
            raise RuntimeError(f"æ‰€æœ‰åµŒå…¥æ¨¡å‹éƒ½å¤±è´¥: ä¸»æ¨¡å‹é”™è¯¯ - {str(e)}, å¤‡ç”¨æ¨¡å‹é”™è¯¯ - {str(fallback_error)}")

async def enhance_with_deepseek(query: str, results: List[Dict]) -> List[Dict]:
    """ç”¨DeepSeekå¢å¼ºç»“æœï¼ˆæ·»åŠ ä¸“ä¸šè§£é‡Šï¼‰"""
    global aiohttp_session
    # æ‡’åŠ è½½aiohttpä¼šè¯
    if aiohttp_session is None:
        aiohttp_session = aiohttp.ClientSession()
    
    api_key = os.getenv("DEEPSEEK_API_KEY")
    api_url = os.getenv("DEEPSEEK_API_URL", "https://api.deepseek.com/v1/chat/completions")
    if not api_key:
        logger.warning("âš ï¸  æœªé…ç½®DeepSeek API Keyï¼Œè·³è¿‡å¢å¼º")
        return results
    
    try:
        prompt = f"""ä¸ºç”Ÿç‰©å…¥ä¾µæŸ¥è¯¢ã€Œ{query}ã€çš„ç»“æœï¼Œæ¯ä¸ªæ¡ç›®æ–°å¢1å¥ä¸“ä¸šè§£é‡Šï¼ˆkeyä¸º"enhanced_info"ï¼‰ï¼Œä»…è¿”å›JSONï¼š
{json.dumps(results, ensure_ascii=False)}"""
        async with aiohttp_session.post(
            api_url,
            headers={
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key}"
            },
            json={
                "model": "deepseek-chat",
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.2,
                "max_tokens": 2048
            },
            timeout=15
        ) as resp:
            if resp.status != 200:
                raise RuntimeError(f"DeepSeek API status: {resp.status}")
            data = await resp.json()
            return json.loads(data["choices"][0]["message"]["content"])
    except Exception as e:
        logger.error(f"âŒ DeepSeekå¢å¼ºå¤±è´¥ï¼š{str(e)}")
        return results

# -------------------------- MCPæœåŠ¡å™¨åˆå§‹åŒ– --------------------------
# åˆå§‹åŒ–MCPæœåŠ¡
mcp = FastMCP("bio-invasion-mongo-server")

# -------------------------- æ ¸å¿ƒå·¥å…·å‡½æ•°ï¼ˆä»enhanced_query_server.pyå¯¼å…¥ï¼‰ --------------------------
def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:
    """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))

def rerank_results(query_text: str, results: List[Tuple[Dict, float]], tokenizer, model) -> List[Tuple[Dict, float]]:
    """ä½¿ç”¨rerankeræ¨¡å‹å¯¹ç»“æœè¿›è¡Œé‡æ’åº"""
    try:
        pairs = [(query_text, doc['metadata'].get('title', '') + " " + 
                 doc['metadata'].get('abstract', '')) for doc, _ in results]
        
        with torch.no_grad():
            # å‡å°‘æœ€å¤§é•¿åº¦ä»¥é™ä½GPUå†…å­˜ä½¿ç”¨
            inputs = tokenizer(
                pairs,
                padding='max_length',
                truncation=True,
                return_tensors='pt',
                max_length=256,  # ä»512å‡å°‘åˆ°256
                pad_to_multiple_of=8
            )
            
            # åˆ†æ‰¹å¤„ç†ä»¥é¿å…å†…å­˜æº¢å‡º
            batch_size = 4  # å‡å°‘batch size
            scores = []
            for i in range(0, len(pairs), batch_size):
                batch_inputs = {k: v[i:i+batch_size].to(model.device) 
                              for k, v in inputs.items()}
                batch_scores = model(**batch_inputs, return_dict=True).logits.view(-1,).float()
                scores.append(batch_scores.cpu())
            
            scores = torch.cat(scores)
    except Exception as e:
        logger.error(f"é‡æ’åºå¤±è´¥: {str(e)}")
        return results
        
    # å°†åˆ†æ•°è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
    scores = torch.sigmoid(scores).numpy()
    
    # æ›´æ–°ç»“æœåˆ†æ•°(æé«˜rerankeræƒé‡)
    reranked = [(doc, sim * 0.3 + score * 0.7)  # 30%åŸå§‹ç›¸ä¼¼åº¦ + 70%rerankeråˆ†æ•°
               for (doc, sim), score in zip(results, scores)]
    
    # æŒ‰æ–°åˆ†æ•°æ’åº
    reranked.sort(key=lambda x: x[1], reverse=True)
    return reranked

def enhance_results_with_llm(query_text: str, original_docs: List[Tuple[Dict, float, Optional[Dict]]]) -> Optional[str]:
    """ä½¿ç”¨DeepSeek APIä¼˜åŒ–è¾“å‡ºç»“æœï¼Œç»“åˆåŸæ–‡æ¡£å†…å®¹"""
    DEEPSEEK_API_KEY = os.getenv('DEEPSEEK_API_KEY')
    DEEPSEEK_API_URL = os.getenv('DEEPSEEK_API_URL')
    
    if not DEEPSEEK_API_KEY:
        logger.warning("âš ï¸  æœªé…ç½®DeepSeek API Keyï¼Œè·³è¿‡ç»“æœä¼˜åŒ–")
        return None
    
    headers = {
        "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
        "Content-Type": "application/json"
    }
    
    # å‡†å¤‡åŒ…å«åŸæ–‡æ¡£å†…å®¹çš„è¯¦ç»†ç»“æœæ‘˜è¦
    results_summary = "\n".join(
        f"ç»“æœ{i} (ç›¸ä¼¼åº¦:{sim:.4f}):\n"
        f"- åˆ†æ®µç¼–å·: {original_doc['chunk_number'] if original_doc else 'æ— '}\n"
        f"- æ¥æºæ–‡ä»¶: {original_doc['source'] if original_doc else 'æœªçŸ¥'}\n"
        f"- å†…å®¹é¢„è§ˆ: {original_doc['content'][:200] + '...' if original_doc and len(original_doc['content']) > 200 else original_doc['content'] if original_doc else 'æ— å†…å®¹'}"
        for i, (doc, sim, original_doc) in enumerate(original_docs, 1)
    )
    
    payload = {
        "model": "deepseek-chat",
        "messages": [
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç”Ÿç‰©å…¥ä¾µç ”ç©¶ä¸“å®¶ã€‚è¯·åŸºäºæ£€ç´¢åˆ°çš„å®é™…æ–‡æ¡£å†…å®¹ï¼Œç”Ÿæˆå…³äºæŸ¥è¯¢ä¸»é¢˜çš„æ·±åº¦æ•´åˆåˆ†ææŠ¥å‘Šã€‚æŠ¥å‘Šè¦æ±‚ï¼š\n\n1. å…³é”®ä¿¡æ¯æå–ï¼šå‡†ç¡®æå–æ–‡æ¡£ä¸­çš„äº‹å®æ•°æ®ã€æ—¶é—´ã€åœ°ç‚¹ã€å½±å“èŒƒå›´ç­‰æ ¸å¿ƒä¿¡æ¯\n2. å†…å®¹æ·±åº¦åˆ†æï¼šåˆ†ææ–‡æ¡£é—´çš„å…³è”æ€§ã€æ•°æ®ä¸€è‡´æ€§ã€ç ”ç©¶è¶‹åŠ¿\n3. ä¸“ä¸šè§è§£ï¼šæä¾›åŸºäºæ–‡æ¡£è¯æ®çš„ä¸“ä¸šåˆ¤æ–­å’Œé£é™©è¯„ä¼°\n4. ç»“æ„åŒ–è¾“å‡ºï¼šä½¿ç”¨æ¸…æ™°çš„ç« èŠ‚ç»“æ„ï¼ŒåŒ…æ‹¬æ‘˜è¦ã€åˆ†æã€ç»“è®ºå’Œå»ºè®®\n5. å‡†ç¡®æ€§ï¼šä¸¥æ ¼åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹ï¼Œä¸æ·»åŠ å¤–éƒ¨çŸ¥è¯†æˆ–å‡è®¾\n\nè¾“å‡ºè¯­è¨€ï¼šä¸­æ–‡\næŠ¥å‘Šé£æ ¼ï¼šå­¦æœ¯ä¸“ä¸šï¼Œæ•°æ®é©±åŠ¨"
            },
            {
                "role": "user",
                "content": f"æŸ¥è¯¢ä¸»é¢˜ï¼š{query_text}\n\næ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£å†…å®¹ï¼ˆæŒ‰ç›¸ä¼¼åº¦æ’åºï¼‰ï¼š\n{results_summary}\n\nè¯·åŸºäºä»¥ä¸Šå®é™…æ–‡æ¡£å†…å®¹ï¼Œç”Ÿæˆä¸€ä»½ä¸“ä¸šçš„æ•´åˆåˆ†ææŠ¥å‘Šã€‚è¦æ±‚ï¼š\n- ä¸¥æ ¼åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹è¿›è¡Œåˆ†æ\n- æå–å…³é”®æ•°æ®å’Œäº‹å®ä¿¡æ¯\n- åˆ†æä¸åŒæ–‡æ¡£é—´çš„å…³è”å’Œä¸€è‡´æ€§\n- è¯„ä¼°ä¿¡æ¯çš„å®Œæ•´æ€§å’Œå¯é æ€§\n- æä¾›ä¸“ä¸šçš„ç»“è®ºå’Œå»ºè®®\n- ä½¿ç”¨æ¸…æ™°çš„ç« èŠ‚ç»“æ„ç»„ç»‡å†…å®¹"
            }
        ],
        "temperature": 0.2,
        "max_tokens": 1500
    }
    
    try:
        response = requests.post(DEEPSEEK_API_URL, headers=headers, json=payload)
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]
    except Exception as e:
        logger.error(f"ç»“æœä¼˜åŒ–å¤±è´¥: {str(e)}")
        return None

@mcp.tool()
async def enhanced_natural_language_query(
    query_text: str,
    db_name: Optional[str] = None,
    collection_name: Optional[str] = None,
    limit: int = 5,
    use_reranker: bool = True,
    enhance_output: bool = False
) -> Dict:
    """å¢å¼ºç‰ˆè‡ªç„¶è¯­è¨€æŸ¥è¯¢MongoDBæ•°æ®åº“
    
    å®Œæ•´æµç¨‹ï¼šç”¨æˆ·è¾“å…¥æ–‡æœ¬ â†’ æ–‡æœ¬è½¬å‘é‡ â†’ å‘é‡æŸ¥è¯¢ â†’ é‡æ’åº â†’ æºæ–‡æœ¬æŸ¥æ‰¾ â†’ ä¼˜åŒ–è¾“å‡º
    
    Args:
        query_text (str): è‡ªç„¶è¯­è¨€æŸ¥è¯¢æ–‡æœ¬
        db_name (str, optional): æ•°æ®åº“å
        collection_name (str, optional): é›†åˆå
        limit (int): è¿”å›ç»“æœæ•°é‡ï¼ˆé»˜è®¤5ï¼‰
        use_reranker (bool): æ˜¯å¦ä½¿ç”¨é‡æ’åºæ¨¡å‹ï¼ˆé»˜è®¤Trueï¼‰
        enhance_output (bool): æ˜¯å¦ä½¿ç”¨AIä¼˜åŒ–è¾“å‡ºï¼ˆé»˜è®¤Falseï¼‰
    
    Returns:
        Dict: åŒ…å«å®Œæ•´æŸ¥è¯¢ç»“æœçš„å­—å…¸
    """
    client = None
    try:
        # å‚æ•°æ ¡éªŒ
        if not query_text.strip():
            return {"status": "error", "msg": "å¿…é¡»æä¾›query_textå‚æ•°"}
        
        # ç¡®å®šä½¿ç”¨çš„æ•°æ®åº“å’Œé›†åˆ
        target_db = db_name or os.getenv('MONGO_DB_NAME', 'ä¸­å›½ç”Ÿç‰©å…¥ä¾µç ”ç©¶')
        target_collection = collection_name or os.getenv('MONGO_COLLECTION', 'ç”Ÿç‰©å…¥ä¾µç ”ç©¶')
        
        # è·å–æŸ¥è¯¢å‘é‡
        try:
            query_vector = await get_embedding(query_text)
            logger.info(f"âœ… æ–‡æœ¬è½¬å‘é‡æˆåŠŸï¼š{query_text[:50]}...")
        except Exception as e:
            logger.error(f"æ–‡æœ¬è½¬å‘é‡å¤±è´¥: {str(e)}")
            return {"status": "error", "msg": f"æ–‡æœ¬è½¬å‘é‡å¤±è´¥: {str(e)}"}
        
        # åˆ›å»ºæ–°çš„MongoDBè¿æ¥
        try:
            client = MongoClient(os.getenv('MONGO_URI', 'mongodb://localhost:27017/'))
            db = client[target_db]
            collection = db[target_collection]
            logger.info(f"âœ… è¿æ¥åˆ°æ•°æ®åº“ï¼š{target_db}.{target_collection}")
        except Exception as e:
            logger.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}")
            return {"status": "error", "msg": f"æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}"}
        
        # æ‰§è¡Œå‘é‡æŸ¥è¯¢ - ä½¿ç”¨MongoDBå‘é‡ç´¢å¼•ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        results = []
        try:
            # é¦–å…ˆæ£€æŸ¥æ•°æ®åº“ä¸­æœ‰å¤šå°‘æ–‡æ¡£
            total_docs = collection.count_documents({})
            logger.info(f"ğŸ“Š æ•°æ®åº“ä¸­æ€»æ–‡æ¡£æ•°: {total_docs}")
            
            # æ£€æŸ¥å‰å‡ ä¸ªæ–‡æ¡£çš„ç»“æ„
            sample_docs = list(collection.find().limit(3))
            logger.info(f"ğŸ“‹ æ ·æœ¬æ–‡æ¡£ç»“æ„: {[list(doc.keys()) for doc in sample_docs]}")
            
            if "vector_index" in collection.index_information():
                # ä½¿ç”¨å‘é‡ç´¢å¼•è¿›è¡Œé«˜æ•ˆæŸ¥è¯¢
                pipeline = [
                    {
                        "$vectorSearch": {
                            "index": "vector_index",
                            "queryVector": query_vector,
                            "path": "embedding",
                            "limit": limit * 2  # å–2å€ç”¨äºé‡æ’åº
                        }
                    },
                    {"$project": {"embedding": 0, "vector_score": {"$meta": "vectorSearchScore"}}}
                ]
                vector_results = list(collection.aggregate(pipeline))
                
                # è½¬æ¢ä¸ºä¸enhanced_query_server.pyå…¼å®¹çš„æ ¼å¼
                for doc in vector_results:
                    similarity = doc.get("vector_score", 0.0)
                    # ç¡®ä¿æ–‡æ¡£æœ‰metadataå­—æ®µï¼Œå¦‚æœæ²¡æœ‰åˆ™åˆ›å»ºç©ºå­—å…¸
                    if 'metadata' not in doc:
                        doc['metadata'] = {}
                    results.append((doc, similarity))
                logger.info(f"âœ… ä½¿ç”¨å‘é‡ç´¢å¼•æŸ¥è¯¢æˆåŠŸï¼Œæ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
            else:
                # å›é€€åˆ°çº¿æ€§æ‰«æï¼ˆå…¼å®¹enhanced_query_server.pyçš„é€»è¾‘ï¼‰
                logger.warning("âš ï¸  æ— vector_indexï¼Œä½¿ç”¨åŸºç¡€æŸ¥è¯¢")
                docs_with_vector = 0
                for doc in collection.find({}):
                    # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦æœ‰dataå­—æ®µï¼ˆå‘é‡æ•°æ®ï¼‰
                    if 'data' in doc:
                        try:
                            docs_with_vector += 1
                            # ç¡®ä¿query_vectoræ˜¯numpyæ•°ç»„
                            query_vec = np.array(query_vector)
                            doc_vec = np.array(doc['data'])
                            similarity = cosine_similarity(query_vec, doc_vec)
                            results.append((doc, similarity))
                        except Exception as e:
                            logger.warning(f"æ–‡æ¡£ {doc.get('source', 'unknown')} å‘é‡è®¡ç®—å¤±è´¥: {str(e)}")
                            continue  # è·³è¿‡è¿™ä¸ªæ–‡æ¡£ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª
                
                logger.info(f"ğŸ“Š æœ‰dataå­—æ®µï¼ˆå‘é‡ï¼‰çš„æ–‡æ¡£æ•°: {docs_with_vector}")
                logger.info(f"âœ… åŸºç¡€æŸ¥è¯¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
        except Exception as e:
            logger.error(f"MongoDBæŸ¥è¯¢å¤±è´¥: {str(e)}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return {"status": "error", "msg": f"MongoDBæŸ¥è¯¢å¤±è´¥: {str(e)}"}
        
        # åˆå§‹æ’åº
        results.sort(key=lambda x: x[1], reverse=True)
        
        # é‡æ’åº
        if use_reranker and reranker_tokenizer and reranker_model and query_text:
            try:
                results = rerank_results(query_text, results, reranker_tokenizer, reranker_model)
                logger.info("âœ… é‡æ’åºå®Œæˆ")
            except Exception as e:
                logger.warning(f"é‡æ’åºå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ’åº: {str(e)}")
        
        # æŸ¥æ‰¾å¯¹åº”çš„åŸæ–‡æ¡£åˆ†æ®µ
        original_docs = []
        for doc, sim in results[:limit]:
            # ä»å‘é‡æ–‡æ¡£çš„sourceå­—æ®µæå–ç¼–å·
            vector_source = doc.get('source', '')
            if vector_source.startswith('ias_cn_') and vector_source.endswith('.npy'):
                # æå–ç¼–å·å¹¶æ„å»ºå¯¹åº”çš„åŸæ–‡æ¡£åˆ†æ®µæ–‡ä»¶å
                chunk_number = vector_source.replace('ias_cn_', '').replace('.npy', '')
                try:
                    chunk_number = int(chunk_number)
                    # æŸ¥æ‰¾å¯¹åº”çš„åŸæ–‡æ¡£åˆ†æ®µ
                    original_doc = collection.find_one({
                        'chunk_number': chunk_number,
                        'file_type': 'markdown_chunk'
                    })
                    original_docs.append((doc, sim, original_doc))
                except ValueError:
                    original_docs.append((doc, sim, None))
            else:
                original_docs.append((doc, sim, None))
        
        # æ„å»ºç»“æœ
        formatted_results = []
        for i, (doc, sim, original_doc) in enumerate(original_docs, 1):
            result_data = {
                "rank": i,
                "similarity_score": float(sim),
                "relevance_level": "é«˜åº¦ç›¸å…³" if sim > 0.8 else "ä¸­ç­‰ç›¸å…³" if sim > 0.5 else "ä½ç›¸å…³",
                "vector_metadata": doc.get('metadata', {}),
                "original_content_available": original_doc is not None
            }
            
            if original_doc:
                result_data.update({
                    "chunk_number": original_doc.get('chunk_number'),
                    "source_file": original_doc.get('source'),
                    "content_preview": original_doc['content'][:200] + "..." if len(original_doc['content']) > 200 else original_doc['content'],
                    "content_length": len(original_doc['content'])
                })
            
            formatted_results.append(result_data)
        
        # AIä¼˜åŒ–è¾“å‡º
        enhanced_report = None
        if enhance_output and query_text and len(results) > 0:
            try:
                enhanced_report = enhance_results_with_llm(query_text, original_docs[:limit])
            except Exception as e:
                logger.warning(f"AIä¼˜åŒ–è¾“å‡ºå¤±è´¥: {str(e)}")
        
        # è¿”å›å®Œæ•´ç»“æœ
        response_data = {
            "status": "success",
            "query_info": {
                "query_text": query_text,
                "database": target_db,
                "collection": target_collection,
                "total_results": len(results),
                "returned_results": len(formatted_results)
            },
            "results": formatted_results
        }
        
        if enhanced_report:
            response_data["enhanced_report"] = enhanced_report
        
        return response_data
        
    except Exception as e:
        logger.error(f"æŸ¥è¯¢å¤±è´¥: {str(e)}")
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        return {"status": "error", "msg": str(e)}
    finally:
        # ç¡®ä¿æ•°æ®åº“è¿æ¥è¢«æ­£ç¡®å…³é—­
        if client:
            try:
                client.close()
                logger.info("âœ… æ•°æ®åº“è¿æ¥å·²å…³é—­")
            except Exception as e:
                logger.warning(f"å…³é—­æ•°æ®åº“è¿æ¥æ—¶å‡ºé”™: {str(e)}")

# -------------------------- MCPèµ„æºå®šä¹‰ --------------------------
@mcp.resource("config://app-version")
def get_app_version() -> str:
    """è¿”å›åº”ç”¨ç‰ˆæœ¬ä¿¡æ¯"""
    return "ç”Ÿç‰©å…¥ä¾µMongoDBæœåŠ¡å™¨ v1.0.0"

@mcp.resource("config://server-status")
def get_server_status() -> Dict:
    """è¿”å›æœåŠ¡å™¨çŠ¶æ€ä¿¡æ¯"""
    return {
        "status": "running",
        "environment": ENV,
        "mongo_connected": mongo_client is not None,
        "reranker_loaded": reranker_model is not None,
        "timestamp": asyncio.get_event_loop().time() if asyncio.get_event_loop().is_running() else 0
    }

@mcp.resource("data://species/count")
async def get_species_count() -> Dict:
    """è¿”å›æ•°æ®åº“ä¸­ç‰©ç§æ€»æ•°"""
    try:
        if not mongo_col:
            return {"error": "MongoDBè¿æ¥æœªåˆå§‹åŒ–"}
        
        count = await asyncio.get_event_loop().run_in_executor(
            None, lambda: mongo_col.count_documents({})
        )
        return {"total_species": count}
    except Exception as e:
        return {"error": str(e)}

@mcp.resource("data://species/{species_id}")
async def get_species_by_id(species_id: str) -> Dict:
    """æ ¹æ®IDè·å–ç‰¹å®šç‰©ç§ä¿¡æ¯"""
    try:
        if not mongo_col:
            return {"error": "MongoDBè¿æ¥æœªåˆå§‹åŒ–"}
        
        species = await asyncio.get_event_loop().run_in_executor(
            None, lambda: mongo_col.find_one({"_id": species_id}, {"embedding": 0})
        )
        if species:
            return {"status": "found", "data": species}
        else:
            return {"status": "not_found", "species_id": species_id}
    except Exception as e:
        return {"error": str(e)}

@mcp.resource("info://supported-queries")
def get_supported_queries() -> List[str]:
    """è¿”å›æ”¯æŒçš„æŸ¥è¯¢ç±»å‹åˆ—è¡¨"""
    return [
        "ç‰©ç§åŸºæœ¬ä¿¡æ¯æŸ¥è¯¢",
        "å…¥ä¾µè·¯å¾„åˆ†æ", 
        "åˆ†å¸ƒèŒƒå›´æŸ¥è¯¢",
        "é˜²æ²»æªæ–½æŸ¥è¯¢",
        "é£é™©è¯„ä¼°æŸ¥è¯¢"
    ]

# -------------------------- MCPæç¤ºæ¨¡æ¿ --------------------------
@mcp.prompt(name="enhanced_species_query", description="å¢å¼ºç‰ˆè‡ªç„¶è¯­è¨€æŸ¥è¯¢å…¥ä¾µç‰©ç§è¯¦ç»†ä¿¡æ¯")
def enhanced_species_query_prompt(species_name: str) -> Dict:
    """ç”Ÿæˆå¢å¼ºç‰ˆç‰©ç§æŸ¥è¯¢æç¤º
    
    Args:
        species_name (str): å…¥ä¾µç‰©ç§åç§°ï¼ˆå¦‚"çº¢ç«èš"ï¼‰
    
    Returns:
        Dict: MCPæ ‡å‡†æç¤ºæ ¼å¼
    """
    return {
        "description": f"ä½¿ç”¨å¢å¼ºç‰ˆè‡ªç„¶è¯­è¨€æŸ¥è¯¢{species_name}çš„ç”Ÿç‰©å…¥ä¾µè¯¦ç»†ä¿¡æ¯",
        "messages": [
            {
                "role": "user",
                "content": {
                    "type": "text",
                    "text": f"è¯·ä½¿ç”¨å¢å¼ºç‰ˆè‡ªç„¶è¯­è¨€æŸ¥è¯¢åŠŸèƒ½æŸ¥è¯¢{species_name}çš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼š1.ç”Ÿç‰©å­¦ç‰¹æ€§ï¼›2.å…¥ä¾µå†å²ï¼›3.åˆ†å¸ƒèŒƒå›´ï¼›4.è¯†åˆ«ç‰¹å¾ï¼›5.é˜²æ²»æªæ–½"
                }
            }
        ],
        "arguments": {
            "query_text": f"æŸ¥è¯¢{species_name}çš„è¯¦ç»†ä¿¡æ¯",
            "limit": 5,
            "use_reranker": True,
            "enhance_output": True
        }
    }

@mcp.prompt(name="enhanced_control_measures", description="å¢å¼ºç‰ˆæŸ¥è¯¢å…¥ä¾µç‰©ç§é˜²æ²»æªæ–½")
def enhanced_control_measures_prompt(species_name: str) -> Dict:
    """ç”Ÿæˆå¢å¼ºç‰ˆé˜²æ²»æªæ–½æŸ¥è¯¢æç¤º"""
    return {
        "description": f"ä½¿ç”¨å¢å¼ºç‰ˆè‡ªç„¶è¯­è¨€æŸ¥è¯¢{species_name}çš„é˜²æ²»æªæ–½",
        "messages": [
            {
                "role": "user",
                "content": {
                    "type": "text",
                    "text": f"è¯·ä½¿ç”¨å¢å¼ºç‰ˆè‡ªç„¶è¯­è¨€æŸ¥è¯¢åŠŸèƒ½æŸ¥è¯¢{species_name}çš„é˜²æ²»æªæ–½ï¼ŒåŒ…æ‹¬ï¼š1.ç‰©ç†é˜²æ²»ï¼›2.åŒ–å­¦é˜²æ²»ï¼›3.ç”Ÿç‰©é˜²æ²»ï¼›4.ç»¼åˆç­–ç•¥"
                }
            }
        ],
        "arguments": {
            "query_text": f"æŸ¥è¯¢{species_name}çš„é˜²æ²»æªæ–½",
            "limit": 5,
            "use_reranker": True,
            "enhance_output": True
        }
    }

# -------------------------- èµ„æºæ¸…ç† --------------------------
async def cleanup_resources():
    """é‡Šæ”¾å…¨å±€èµ„æº"""
    logger.info("ğŸ›‘ å¼€å§‹é‡Šæ”¾èµ„æº")
    # å…³é—­MongoDBè¿æ¥
    if mongo_client:
        mongo_client.close()
        logger.info("âœ… MongoDBè¿æ¥å·²å…³é—­")
    # å…³é—­aiohttpä¼šè¯
    if aiohttp_session:
        await aiohttp_session.close()
        logger.info("âœ… aiohttpä¼šè¯å·²å…³é—­")

# -------------------------- ä¸»å‡½æ•° --------------------------
if __name__ == "__main__":
    try:
        # ä½¿ç”¨stdioä¼ è¾“æ–¹å¼å¯åŠ¨MCPæœåŠ¡å™¨ï¼ˆä¸å®¢æˆ·ç«¯å…¼å®¹ï¼‰
        logger.info("ğŸš€ å¯åŠ¨MCPæœåŠ¡å™¨...")
        mcp.run(transport="stdio")
    except KeyboardInterrupt:
        logger.info("\nâš ï¸  æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼ˆCtrl+Cï¼‰")
    finally:
        # é‡Šæ”¾èµ„æº
        asyncio.run(cleanup_resources())
        logger.info("âœ… æœåŠ¡å·²å®Œå…¨åœæ­¢")
