import os
import json
import asyncio
import logging
import numpy as np
import requests
import torch
import re
from typing import Dict, List, Optional, Tuple
from pymongo import MongoClient
from dotenv import load_dotenv
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from pprint import pprint

# ä¸¥æ ¼éµå¾ªå®˜æ–¹MCPç¤ºä¾‹çš„å¯¼å…¥æ–¹å¼
from mcp.server.fastmcp import FastMCP

# -------------------------- åŸºç¡€é…ç½® --------------------------
# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    encoding='utf-8'
)
logger = logging.getLogger("enhanced-bio-invasion-mcp")

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()
ENV = os.getenv("MCP_ENV", "development")

# -------------------------- å…¨å±€èµ„æºåˆå§‹åŒ– --------------------------
# MongoDBè¿æ¥
mongo_client = None
mongo_col = None

def init_mongo():
    """åˆå§‹åŒ–MongoDBè¿æ¥"""
    global mongo_client, mongo_col
    mongo_uri = os.getenv('MONGO_URI', 'mongodb://localhost:27017/')
    default_db = os.getenv('MONGO_DB_NAME', 'ä¸­å›½ç”Ÿç‰©å…¥ä¾µç ”ç©¶')
    default_collection = os.getenv('MONGO_COLLECTION', 'ç”Ÿç‰©å…¥ä¾µç ”ç©¶')
    
    try:
        client = MongoClient(mongo_uri, serverSelectionTimeoutMS=5000)
        client.admin.command("ping")  # éªŒè¯è¿æ¥
        db = client[default_db]
        col = db[default_collection]
        logger.info(f"âœ… MongoDBè¿æ¥æˆåŠŸï¼š{default_db}.{default_collection}")
        mongo_client = client
        mongo_col = col
        return True
    except Exception as e:
        logger.error(f"âŒ MongoDBåˆå§‹åŒ–å¤±è´¥ï¼š{str(e)}")
        return False

# é‡æ’åºæ¨¡å‹
reranker_tokenizer = None
reranker_model = None

def init_reranker():
    """åŠ è½½é‡æ’åºæ¨¡å‹ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œé¿å…å†…å­˜é—®é¢˜ï¼‰"""
    global reranker_tokenizer, reranker_model
    model_name = "BAAI/bge-reranker-base"  # ä½¿ç”¨æ›´å°çš„æ¨¡å‹
    
    try:
        # ç®€åŒ–æ¨¡å‹åŠ è½½ï¼Œå‡å°‘å†…å­˜ä½¿ç”¨
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True
        )
        
        model = AutoModelForSequenceClassification.from_pretrained(
            model_name,
            trust_remote_code=True,
            torch_dtype=torch.float32,  # ä½¿ç”¨float32å‡å°‘å…¼å®¹æ€§é—®é¢˜
            device_map="cpu"  # å¼ºåˆ¶ä½¿ç”¨CPUé¿å…GPUå†…å­˜é—®é¢˜
        )
        model.eval()
        
        reranker_tokenizer = tokenizer
        reranker_model = model
        logger.info(f"âœ… é‡æ’åºæ¨¡å‹åŠ è½½æˆåŠŸï¼š{model_name}")
        return True
    except Exception as e:
        logger.warning(f"âš ï¸  é‡æ’åºæ¨¡å‹åŠ è½½å¤±è´¥ï¼š{str(e)}ï¼ˆå°†è·³è¿‡é‡æ’åºæ­¥éª¤ï¼‰")
        return False

# åˆå§‹åŒ–èµ„æº
mongo_initialized = init_mongo()
reranker_initialized = init_reranker()

# -------------------------- æ ¸å¿ƒå·¥å…·å‡½æ•° --------------------------
def text_to_vector(text: str) -> np.ndarray:
    """ä½¿ç”¨OllamaåµŒå…¥æ¨¡å‹å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡"""
    try:
        import ollama
        
        # æ–‡æœ¬å½’ä¸€åŒ–å¤„ç†
        text = text.lower()  # ç»Ÿä¸€å°å†™
        text = re.sub(r'[^\w\s]', '', text)  # å»é™¤æ ‡ç‚¹
        text = re.sub(r'\s+', ' ', text).strip()  # æ ‡å‡†åŒ–ç©ºæ ¼
        
        # ä½¿ç”¨OllamaåµŒå…¥æ¨¡å‹
        embedding_model = os.getenv("EMBEDDING_MODEL", "qwen3-embedding:8b")
        
        try:
            # ä½¿ç”¨ä¸»æ¨¡å‹ç”ŸæˆåµŒå…¥å‘é‡
            response = ollama.embeddings(model=embedding_model, prompt=text)
            vector = np.array(response["embedding"])
            logger.info(f"âœ… ä½¿ç”¨OllamaåµŒå…¥æ¨¡å‹æˆåŠŸ: {embedding_model}")
            return vector
        except Exception as e:
            logger.warning(f"âš ï¸  ä¸»åµŒå…¥æ¨¡å‹ {embedding_model} å¤±è´¥: {str(e)}ï¼Œå°è¯•å¤‡ç”¨æ¨¡å‹")
            
            # å›é€€åˆ°nomic-embed-textæ¨¡å‹
            try:
                response = ollama.embeddings(model="nomic-embed-text", prompt=text)
                vector = np.array(response["embedding"])
                logger.info(f"âœ… ä½¿ç”¨å¤‡ç”¨åµŒå…¥æ¨¡å‹æˆåŠŸ")
                return vector
            except Exception as fallback_error:
                raise RuntimeError(f"æ‰€æœ‰åµŒå…¥æ¨¡å‹éƒ½å¤±è´¥: ä¸»æ¨¡å‹é”™è¯¯ - {str(e)}, å¤‡ç”¨æ¨¡å‹é”™è¯¯ - {str(fallback_error)}")
            
    except ImportError:
        logger.error("âŒ ollamaåº“æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install ollama")
        raise RuntimeError("ollamaåº“æœªå®‰è£…")
    except Exception as e:
        logger.error(f"âŒ æ–‡æœ¬è½¬å‘é‡å¤±è´¥: {str(e)}")
        raise

def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:
    """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))

def rerank_results(query_text: str, results: List[Tuple[Dict, float]], tokenizer, model) -> List[Tuple[Dict, float]]:
    """ä½¿ç”¨rerankeræ¨¡å‹å¯¹ç»“æœè¿›è¡Œé‡æ’åº"""
    try:
        pairs = [(query_text, doc['metadata'].get('title', '') + " " + 
                 doc['metadata'].get('abstract', '')) for doc, _ in results]
        
        with torch.no_grad():
            # å‡å°‘æœ€å¤§é•¿åº¦ä»¥é™ä½GPUå†…å­˜ä½¿ç”¨
            inputs = tokenizer(
                pairs,
                padding='max_length',
                truncation=True,
                return_tensors='pt',
                max_length=256,  # ä»512å‡å°‘åˆ°256
                pad_to_multiple_of=8
            )
            
            # åˆ†æ‰¹å¤„ç†ä»¥é¿å…å†…å­˜æº¢å‡º
            batch_size = 4  # å‡å°‘batch size
            scores = []
            for i in range(0, len(pairs), batch_size):
                batch_inputs = {k: v[i:i+batch_size].to(model.device) 
                              for k, v in inputs.items()}
                batch_scores = model(**batch_inputs, return_dict=True).logits.view(-1,).float()
                scores.append(batch_scores.cpu())
            
            scores = torch.cat(scores)
    except Exception as e:
        logger.error(f"é‡æ’åºå¤±è´¥: {str(e)}")
        return results
        
    # å°†åˆ†æ•°è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
    scores = torch.sigmoid(scores).numpy()
    
    # æ›´æ–°ç»“æœåˆ†æ•°(æé«˜rerankeræƒé‡)
    reranked = [(doc, sim * 0.3 + score * 0.7)  # 30%åŸå§‹ç›¸ä¼¼åº¦ + 70%rerankeråˆ†æ•°
               for (doc, sim), score in zip(results, scores)]
    
    # æŒ‰æ–°åˆ†æ•°æ’åº
    reranked.sort(key=lambda x: x[1], reverse=True)
    return reranked

def enhance_results_with_llm(query_text: str, original_docs: List[Tuple[Dict, float, Optional[Dict]]]) -> Optional[str]:
    """ä½¿ç”¨DeepSeek APIä¼˜åŒ–è¾“å‡ºç»“æœï¼Œç»“åˆåŸæ–‡æ¡£å†…å®¹"""
    DEEPSEEK_API_KEY = os.getenv('DEEPSEEK_API_KEY')
    DEEPSEEK_API_URL = os.getenv('DEEPSEEK_API_URL')
    
    if not DEEPSEEK_API_KEY:
        logger.warning("âš ï¸  æœªé…ç½®DeepSeek API Keyï¼Œè·³è¿‡ç»“æœä¼˜åŒ–")
        return None
    
    headers = {
        "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
        "Content-Type": "application/json"
    }
    
    # å‡†å¤‡åŒ…å«åŸæ–‡æ¡£å†…å®¹çš„è¯¦ç»†ç»“æœæ‘˜è¦
    results_summary = "\n".join(
        f"ç»“æœ{i} (ç›¸ä¼¼åº¦:{sim:.4f}):\n"
        f"- åˆ†æ®µç¼–å·: {original_doc['chunk_number'] if original_doc else 'æ— '}\n"
        f"- æ¥æºæ–‡ä»¶: {original_doc['source'] if original_doc else 'æœªçŸ¥'}\n"
        f"- å†…å®¹é¢„è§ˆ: {original_doc['content'][:200] + '...' if original_doc and len(original_doc['content']) > 200 else original_doc['content'] if original_doc else 'æ— å†…å®¹'}"
        for i, (doc, sim, original_doc) in enumerate(original_docs, 1)
    )
    
    payload = {
        "model": "deepseek-chat",
        "messages": [
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç”Ÿç‰©å…¥ä¾µç ”ç©¶ä¸“å®¶ã€‚è¯·åŸºäºæ£€ç´¢åˆ°çš„å®é™…æ–‡æ¡£å†…å®¹ï¼Œç”Ÿæˆå…³äºæŸ¥è¯¢ä¸»é¢˜çš„æ·±åº¦æ•´åˆåˆ†ææŠ¥å‘Šã€‚æŠ¥å‘Šè¦æ±‚ï¼š\n\n1. å…³é”®ä¿¡æ¯æå–ï¼šå‡†ç¡®æå–æ–‡æ¡£ä¸­çš„äº‹å®æ•°æ®ã€æ—¶é—´ã€åœ°ç‚¹ã€å½±å“èŒƒå›´ç­‰æ ¸å¿ƒä¿¡æ¯\n2. å†…å®¹æ·±åº¦åˆ†æï¼šåˆ†ææ–‡æ¡£é—´çš„å…³è”æ€§ã€æ•°æ®ä¸€è‡´æ€§ã€ç ”ç©¶è¶‹åŠ¿\n3. ä¸“ä¸šè§è§£ï¼šæä¾›åŸºäºæ–‡æ¡£è¯æ®çš„ä¸“ä¸šåˆ¤æ–­å’Œé£é™©è¯„ä¼°\n4. ç»“æ„åŒ–è¾“å‡ºï¼šä½¿ç”¨æ¸…æ™°çš„ç« èŠ‚ç»“æ„ï¼ŒåŒ…æ‹¬æ‘˜è¦ã€åˆ†æã€ç»“è®ºå’Œå»ºè®®\n5. å‡†ç¡®æ€§ï¼šä¸¥æ ¼åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹ï¼Œä¸æ·»åŠ å¤–éƒ¨çŸ¥è¯†æˆ–å‡è®¾\n\nè¾“å‡ºè¯­è¨€ï¼šä¸­æ–‡\næŠ¥å‘Šé£æ ¼ï¼šå­¦æœ¯ä¸“ä¸šï¼Œæ•°æ®é©±åŠ¨"
            },
            {
                "role": "user",
                "content": f"æŸ¥è¯¢ä¸»é¢˜ï¼š{query_text}\n\næ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£å†…å®¹ï¼ˆæŒ‰ç›¸ä¼¼åº¦æ’åºï¼‰ï¼š\n{results_summary}\n\nè¯·åŸºäºä»¥ä¸Šå®é™…æ–‡æ¡£å†…å®¹ï¼Œç”Ÿæˆä¸€ä»½ä¸“ä¸šçš„æ•´åˆåˆ†ææŠ¥å‘Šã€‚è¦æ±‚ï¼š\n- ä¸¥æ ¼åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹è¿›è¡Œåˆ†æ\n- æå–å…³é”®æ•°æ®å’Œäº‹å®ä¿¡æ¯\n- åˆ†æä¸åŒæ–‡æ¡£é—´çš„å…³è”å’Œä¸€è‡´æ€§\n- è¯„ä¼°ä¿¡æ¯çš„å®Œæ•´æ€§å’Œå¯é æ€§\n- æä¾›ä¸“ä¸šçš„ç»“è®ºå’Œå»ºè®®\n- ä½¿ç”¨æ¸…æ™°çš„ç« èŠ‚ç»“æ„ç»„ç»‡å†…å®¹"
            }
        ],
        "temperature": 0.2,
        "max_tokens": 1500
    }
    
    try:
        response = requests.post(DEEPSEEK_API_URL, headers=headers, json=payload)
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]
    except Exception as e:
        logger.error(f"ç»“æœä¼˜åŒ–å¤±è´¥: {str(e)}")
        return None

# -------------------------- MCPæœåŠ¡å™¨åˆå§‹åŒ– --------------------------
# åˆå§‹åŒ–MCPæœåŠ¡
mcp = FastMCP("enhanced-bio-invasion-server")

@mcp.tool()
async def enhanced_natural_language_query(
    query_text: str,
    db_name: Optional[str] = None,
    collection_name: Optional[str] = None,
    limit: int = 5,
    use_reranker: bool = True,
    enhance_output: bool = False
) -> Dict:
    """å¢å¼ºç‰ˆè‡ªç„¶è¯­è¨€æŸ¥è¯¢MongoDBæ•°æ®åº“
    
    å®Œæ•´æµç¨‹ï¼šç”¨æˆ·è¾“å…¥æ–‡æœ¬ â†’ æ–‡æœ¬è½¬å‘é‡ â†’ å‘é‡æŸ¥è¯¢ â†’ é‡æ’åº â†’ æºæ–‡æœ¬æŸ¥æ‰¾ â†’ ä¼˜åŒ–è¾“å‡º
    
    Args:
        query_text (str): è‡ªç„¶è¯­è¨€æŸ¥è¯¢æ–‡æœ¬
        db_name (str, optional): æ•°æ®åº“å
        collection_name (str, optional): é›†åˆå
        limit (int): è¿”å›ç»“æœæ•°é‡ï¼ˆé»˜è®¤5ï¼‰
        use_reranker (bool): æ˜¯å¦ä½¿ç”¨é‡æ’åºæ¨¡å‹ï¼ˆé»˜è®¤Trueï¼‰
        enhance_output (bool): æ˜¯å¦ä½¿ç”¨AIä¼˜åŒ–è¾“å‡ºï¼ˆé»˜è®¤Falseï¼‰
    
    Returns:
        Dict: åŒ…å«å®Œæ•´æŸ¥è¯¢ç»“æœçš„å­—å…¸
    """
    try:
        # å‚æ•°æ ¡éªŒ
        if not query_text.strip():
            return {"status": "error", "msg": "å¿…é¡»æä¾›query_textå‚æ•°"}
        
        if mongo_col is None:
            return {"status": "error", "msg": "MongoDBè¿æ¥æœªåˆå§‹åŒ–"}
        
        # ç¡®å®šä½¿ç”¨çš„æ•°æ®åº“å’Œé›†åˆ
        target_db = db_name or os.getenv('MONGO_DB_NAME', 'ä¸­å›½ç”Ÿç‰©å…¥ä¾µç ”ç©¶')
        target_collection = collection_name or os.getenv('MONGO_COLLECTION', 'ç”Ÿç‰©å…¥ä¾µç ”ç©¶')
        
        # è·å–æŸ¥è¯¢å‘é‡
        try:
            query_vector = text_to_vector(query_text)
            logger.info(f"âœ… æ–‡æœ¬è½¬å‘é‡æˆåŠŸï¼š{query_text[:50]}...")
        except Exception as e:
            return {"status": "error", "msg": f"æ–‡æœ¬è½¬å‘é‡å¤±è´¥: {str(e)}"}
        
        # åˆ‡æ¢åˆ°ç›®æ ‡æ•°æ®åº“å’Œé›†åˆ
        try:
            client = MongoClient(os.getenv('MONGO_URI', 'mongodb://localhost:27017/'))
            db = client[target_db]
            collection = db[target_collection]
            logger.info(f"âœ… åˆ‡æ¢åˆ°æ•°æ®åº“ï¼š{target_db}.{target_collection}")
        except Exception as e:
            return {"status": "error", "msg": f"æ•°æ®åº“åˆ‡æ¢å¤±è´¥: {str(e)}"}
        
        # æ‰§è¡Œå‘é‡æŸ¥è¯¢ - ç›´æ¥æŸ¥è¯¢MongoDBä¸­çš„å‘é‡æ•°æ®
        results = []
        for doc in collection.find({}):
            if 'data' in doc:
                similarity = cosine_similarity(np.array(doc['data']), query_vector)
                results.append((doc, similarity))
        
        # åˆå§‹æ’åº
        results.sort(key=lambda x: x[1], reverse=True)
        
        # é‡æ’åº
        if use_reranker and reranker_tokenizer and reranker_model and query_text:
            results = rerank_results(query_text, results, reranker_tokenizer, reranker_model)
            logger.info("âœ… é‡æ’åºå®Œæˆ")
        
        # æŸ¥æ‰¾å¯¹åº”çš„åŸæ–‡æ¡£åˆ†æ®µ
        original_docs = []
        for doc, sim in results[:limit]:
            # ä»å‘é‡æ–‡æ¡£çš„sourceå­—æ®µæå–ç¼–å·
            vector_source = doc.get('source', '')
            if vector_source.startswith('ias_cn_') and vector_source.endswith('.npy'):
                # æå–ç¼–å·å¹¶æ„å»ºå¯¹åº”çš„åŸæ–‡æ¡£åˆ†æ®µæ–‡ä»¶å
                chunk_number = vector_source.replace('ias_cn_', '').replace('.npy', '')
                try:
                    chunk_number = int(chunk_number)
                    # æŸ¥æ‰¾å¯¹åº”çš„åŸæ–‡æ¡£åˆ†æ®µ
                    original_doc = collection.find_one({
                        'chunk_number': chunk_number,
                        'file_type': 'markdown_chunk'
                    })
                    original_docs.append((doc, sim, original_doc))
                except ValueError:
                    original_docs.append((doc, sim, None))
            else:
                original_docs.append((doc, sim, None))
        
        # æ„å»ºç»“æœ
        formatted_results = []
        for i, (doc, sim, original_doc) in enumerate(original_docs, 1):
            result_data = {
                "rank": i,
                "similarity_score": float(sim),
                "relevance_level": "é«˜åº¦ç›¸å…³" if sim > 0.8 else "ä¸­ç­‰ç›¸å…³" if sim > 0.5 else "ä½ç›¸å…³",
                "vector_metadata": doc['metadata'],
                "original_content_available": original_doc is not None
            }
            
            if original_doc:
                result_data.update({
                    "chunk_number": original_doc.get('chunk_number'),
                    "source_file": original_doc.get('source'),
                    "content_preview": original_doc['content'][:200] + "..." if len(original_doc['content']) > 200 else original_doc['content'],
                    "content_length": len(original_doc['content'])
                })
            
            formatted_results.append(result_data)
        
        # AIä¼˜åŒ–è¾“å‡º
        enhanced_report = None
        if enhance_output and query_text and len(results) > 0:
            enhanced_report = enhance_results_with_llm(query_text, original_docs[:limit])
        
        # è¿”å›å®Œæ•´ç»“æœ
        response_data = {
            "status": "success",
            "query_info": {
                "query_text": query_text,
                "database": target_db,
                "collection": target_collection,
                "total_results": len(results),
                "returned_results": len(formatted_results)
            },
            "results": formatted_results
        }
        
        if enhanced_report:
            response_data["enhanced_report"] = enhanced_report
        
        return response_data
        
    except Exception as e:
        logger.error(f"æŸ¥è¯¢å¤±è´¥: {str(e)}")
        return {"status": "error", "msg": str(e)}
    finally:
        if 'client' in locals():
            client.close()

# -------------------------- èµ„æºæ¸…ç† --------------------------
async def cleanup_resources():
    """é‡Šæ”¾å…¨å±€èµ„æº"""
    logger.info("ğŸ›‘ å¼€å§‹é‡Šæ”¾èµ„æº")
    # å…³é—­MongoDBè¿æ¥
    if mongo_client:
        mongo_client.close()
        logger.info("âœ… MongoDBè¿æ¥å·²å…³é—­")

# -------------------------- ä¸»å‡½æ•° --------------------------
if __name__ == "__main__":
    try:
        # å¯åŠ¨MCPæœåŠ¡å™¨
        logger.info("ğŸš€ å¯åŠ¨å¢å¼ºç‰ˆç”Ÿç‰©å…¥ä¾µMCPæœåŠ¡å™¨...")
        print("ğŸš€ å¢å¼ºç‰ˆç”Ÿç‰©å…¥ä¾µMCPæœåŠ¡å™¨å¯åŠ¨ä¸­...")
        mcp.run(transport="stdio")
    except KeyboardInterrupt:
        logger.info("\nâš ï¸  æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼ˆCtrl+Cï¼‰")
        print("\nâš ï¸  æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼ˆCtrl+Cï¼‰")
    except Exception as e:
        logger.error(f"æœåŠ¡å™¨å¯åŠ¨å¤±è´¥: {str(e)}")
        print(f"âŒ æœåŠ¡å™¨å¯åŠ¨å¤±è´¥: {str(e)}")
    finally:
        # é‡Šæ”¾èµ„æº
        asyncio.run(cleanup_resources())
        logger.info("âœ… æœåŠ¡å·²å®Œå…¨åœæ­¢")
        print("âœ… æœåŠ¡å·²å®Œå…¨åœæ­¢")
